{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install getuseragent\n",
    "# ! pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173cf10",
   "metadata": {},
   "source": [
    "## DataIngestion_workflow\n",
    "\n",
    "La data ingestion è un processo che consiste nell'ottenere e importare dati, per utilizzo immediato o l'archiviazione in un database o altri sistemi di gestione dei dati.\n",
    "\n",
    "L'obiettivo è quello di collezionare le informazioni delle offerte di lavoro nel modo più dettagliato possibile sito www.glassdoor.com .\n",
    "\n",
    "Una volta definite le funzioni per procedere con lo scraping, ci saranno due faso di scraping:\n",
    "1. Scraping delle offerte di lavoro \n",
    "2. Scraping delle aziende che offrono un lavoro\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/58252186/220355428-2a80bfa3-6174-4a7a-b94a-95bc295657e3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86d661",
   "metadata": {},
   "source": [
    "### 1.Scraping delle offerte di lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cff3a8",
   "metadata": {},
   "source": [
    "Questa prima fase si occuperà di fare scraping sulle offerte di lavoro relative ad un determinato link.\n",
    "(es [offerte lavoro amsterdam]('https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0'))\n",
    "\n",
    "\n",
    "Se il link presenta più pagine relative alle offerte cercate, il sistema andrà a fare lo scraping su tutte le pagine sino ad arrivare alla 30°.  \n",
    "Dopo aver avviato la navigazione web il sistema rifiuterà i cookies e inizierà lo scraping per tutte le pagine presenti nel link.  \n",
    "Una volta completato lo scraping verrà creato un dataframe con le informazioni raccolte. Il dataframe presenterà queste colonne:\n",
    "- company: nome dell'azienda\n",
    "- job_title: Posizione lavorativa offerta\n",
    "- location: Sede dell'offerta di lavoro\n",
    "- company_rating: rating dell'azienda (secondo gli utenti di Glassdoor)\n",
    "- job_age: giorni passati dalla pubblicazione dell'offerta\n",
    "- job_link: link dell'azienda che ha pubblicato il lavoro\n",
    "- scraping_date: verrà aggiunta la data in cui è stato compiuto il processo di scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e09e68",
   "metadata": {},
   "source": [
    "Si farà adesso lo scraping delle prime due pagine del link [offerte lavoro amsterdam]('https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94026bb0",
   "metadata": {},
   "source": [
    "### 2. Scraping della scheda di valutazione delle aziende che offrono un lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b40625",
   "metadata": {},
   "source": [
    "Una volta completato la prima fase di scraping, e' possibile passare alla seconda.  \n",
    "\n",
    "Questa seconda fase prevede la ricerca di informazioni delle aziende che offrono un lavoro, queste verrano cercate nella sezione dedicata creata da Glassdoor.  \n",
    "\n",
    "Si compierà uno scraping delle informazioni generali e delle valutazioni date dagli utenti di Glassdoor.  \n",
    "\n",
    "\n",
    "Per fare questo utilizziamo la colonna \"job_link\" (*link dell'azienda che ha pubblicato il lavoro*) che trasformiamo in una lista da passare alla funzione \"scraping_company_page\".\n",
    "\n",
    "\n",
    "Una volta completato lo scraping verrà creato un dataframe con le informazioni raccolte. Il dataframe presenterà queste colonne:\n",
    "- Opportunità di carriera (rating)\n",
    "- Stipendio e benefit (rating)\n",
    "- Cultura e valori (rating)\n",
    "- Dirigenti senior (rating)\n",
    "- Equilibrio lavoro/vita privata (rating)\n",
    "- Sede centrale\t\n",
    "- Dimensioni\t\n",
    "- Fondata nel\t\n",
    "- Tipo\t\n",
    "- Settore Segmento\t\n",
    "- Entrate\n",
    "- job_link (chiave primaria)\n",
    "\n",
    "Sarà possibile unire i due dataframe con la chiave job_link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb9d6a",
   "metadata": {},
   "source": [
    "### Definizione delle funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5684ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "from datetime import date\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "# BeautifulSoup e Request\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from getuseragent import UserAgent\n",
    "\n",
    "# selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "useragent = UserAgent()\n",
    "\n",
    "theuseragent = useragent.Random()\n",
    "headers = {'User-Agent': theuseragent}\n",
    "header = {\n",
    "    \"user-agent\": theuseragent ,\n",
    "    'referer':'https://www.google.com/'\n",
    "}\n",
    "\n",
    "def allow_cookies(driver) :\n",
    "    print ('Decline Cookies')\n",
    "    button_setting_cookies = driver.find_element (By.CSS_SELECTOR , \"button.cookie-setting-link\")\n",
    "    button_setting_cookies.click ()\n",
    "    time.sleep (np.random.choice ([x / 10 for x in range (7 , 22)]))\n",
    "    button_confirm_cookies = driver.find_element (By.CSS_SELECTOR , \"button.save-preference-btn-handler\")\n",
    "    # Conferma le mie scelte\n",
    "    button_confirm_cookies.click ()\n",
    "\n",
    "\n",
    "def next_page(driver) :\n",
    "    print ('Changing page')\n",
    "    next_button = driver.find_element (By.CSS_SELECTOR , \"button.nextButton\")\n",
    "    next_button.click ()\n",
    "    url = driver.current_url\n",
    "\n",
    "\n",
    "def parse_url(url) :\n",
    "    company_name = []\n",
    "    job_title = []\n",
    "    location = []\n",
    "    company_rating = []\n",
    "    job_age = []\n",
    "    job_link = []\n",
    "\n",
    "    r = requests.get (url , headers = header)\n",
    "    soup = BeautifulSoup (r.text , 'html.parser')\n",
    "    job_search_div = soup.select ('div#JobSearch') [0]\n",
    "    a = job_search_div.select ('div#PageBodyContents')\n",
    "    b = a [0].select ('div#JobResults')\n",
    "    c = b [0].select ('article#MainCol')\n",
    "    len_li = len (c [0].find_all ('li' , class_ = 'react-job-listing'))\n",
    "    lis = c [0].find_all ('li' , class_ = 'react-job-listing')\n",
    "\n",
    "    len_li = len (lis)\n",
    "    df = pd.DataFrame ()\n",
    "\n",
    "    for e in lis [0 :len_li] :\n",
    "        try :\n",
    "            company_name.append (e.find ('div').find ('a') ['title'])\n",
    "        except :\n",
    "            company_name.append (None)\n",
    "        try :\n",
    "            job_title.append (e ['data-normalize-job-title'])\n",
    "        except :\n",
    "            job_title.append (None)\n",
    "        try :\n",
    "            location.append (e ['data-job-loc'])\n",
    "        except :\n",
    "            location.append (None)\n",
    "        try :\n",
    "            company_rating.append (e.find ('span' , class_ = 'css-2lqh28'))  ### da controllare\n",
    "        except :\n",
    "            company_rating.append (None)\n",
    "        try :\n",
    "            job_age.append (e.find (\"div\" , {\"data-test\" : \"job-age\"}).text)\n",
    "        except :\n",
    "            job_link.append (None)\n",
    "        try :\n",
    "            link = \"https://www.glassdoor.it\" + e.find (\"a\" , {\"data-test\" : \"job-link\"}).get (\"href\")\n",
    "            job_link.append (link)\n",
    "        except :\n",
    "            job_link.append (None)\n",
    "\n",
    "    df ['company'] = company_name\n",
    "    df ['job_title'] = job_title\n",
    "    df ['location'] = location\n",
    "    df ['company_rating'] = company_rating\n",
    "    df ['job_age'] = job_age\n",
    "    df ['job_link'] = job_link\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def scraping_job_page(base_url , n_page=None) :\n",
    "    start_time = time.time ()\n",
    "    time_sleep = 0\n",
    "    df_append = pd.DataFrame ()\n",
    "    driver = webdriver.Chrome (r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\")\n",
    "    # base_url = 'https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0'\n",
    "    driver.get (base_url)\n",
    "    driver.maximize_window()\n",
    "    a = np.random.choice ([x for x in range (3 , 5)])\n",
    "    time_sleep = time_sleep + a\n",
    "    time.sleep (a)\n",
    "    url = driver.current_url\n",
    "    \n",
    "    allow_cookies (driver)\n",
    "    a = np.random.choice ([x for x in range (3 , 5)])\n",
    "    time_sleep = time_sleep + a\n",
    "    time.sleep (a)\n",
    "    r = requests.get (url , headers = header)\n",
    "\n",
    "    text = BeautifulSoup (r.text , 'html.parser').find ('div' , attrs = {'class' : 'paginationFooter'}).text\n",
    "\n",
    "    int_list = []\n",
    "    for e in re.findall (r'-?\\d+\\.?\\d*' , text) :\n",
    "        int_list.append (int (e))\n",
    "\n",
    "    if n_page == None :\n",
    "        n_page = max (int_list)\n",
    "    # n_page = 5\n",
    "    print ('Numero di pagine sui cui fare scraping: ' + str (n_page))\n",
    "    a = np.random.choice ([x for x in range (3 , 5)])\n",
    "    time_sleep = time_sleep + a\n",
    "    time.sleep (a)\n",
    "\n",
    "    for page in range (n_page) :\n",
    "        print (str (page + 1) + '/' + str (n_page))\n",
    "        a = np.random.choice ([x for x in range (3 , 5)])\n",
    "        time_sleep = time_sleep + a\n",
    "        time.sleep (a)\n",
    "        next_page (driver)\n",
    "        url = driver.current_url\n",
    "        if page == 0 :\n",
    "            time.sleep (5)\n",
    "            close_button = driver.find_element (By.CSS_SELECTOR , \"svg.SVGInline-svg.modal_closeIcon-svg\")\n",
    "            close_button.click ()\n",
    "        df = parse_url (url)\n",
    "        df_append = df_append.append (df).reset_index (drop = True)\n",
    "\n",
    "    driver.quit ()\n",
    "    end_time = time.time ()\n",
    "    print ('Done!')\n",
    "    #print ('Total pages scraped:' , n_page , '\\n')\n",
    "    print (\"Runtime:\" , round (end_time - start_time) , \"seconds\" + '\\nTime sleep:' , time_sleep , 'seconds')\n",
    "\n",
    "    return df_append\n",
    "\n",
    "\n",
    "def scraping_company_page(links , end_n_page=None , start_n_page=None) :\n",
    "    start_time = time.time ()\n",
    "    df_append_2 = pd.DataFrame ()\n",
    "    time_sleep = 0\n",
    "    n_link = 0\n",
    "    ## lista dei link\n",
    "\n",
    "    # base_url = 'https://www.glassdoor.it/job-listing/field-service-engineer-technician-lexas-JV_IC2802269_KO0,33_KE34,39.htm?jl=1008135983566&pos=101&ao=1110586&s=58&guid=000001861873f6d389d965e51ce717e2&src=GD_JOB_AD&t=SR&vt=w&uido=FEB79CBEB143B609D33C645CD13E2F4B&ea=1&cs=1_524e761d&cb=1675447498725&jobListingId=1008135983566&cpc=65CC663E25211861&jrtk=3-0-1goc77to1g2ev801-1goc77tp0i6id800-7cbed71e05432cdf--6NYlbfkN0CVO0F7mWis5ReNIXvK0Cy97GKSpj_H8mHyNoiV7tLwhxrGQFeFbXfrFFwDAnfvPXeiJe5SavTtAEQpKcpYVReYHZsV-4ZX7UeAkoBb0f_WCVWviQdPDhB0WcxVHddsJTu6CPWu9hRPncXvGLdy3ZffF5b3aOd7vp19QcNQdw0qQd1bkijbQHvL2CZX_Cxp4BGS1Sk8JgAjiz75HrAHRuR5hA9kjnxafzWfGAAOJBSKybBbJtFcKCvWC2Py0-IgF36KHcIY5QbFzm8TqI0WJJ75VyN8D93fcG7Ikeu9ECT-vBbPKtsVv7AOpI6elTa0KTJqDzeYeI39a8bHa5BUUOjrQ1rsJFwMGLJrLrbqXOIObHs1pSu_fpr-FcNbNcmvwdLQufgm_hOEka1AR5pS2jw3Kd3MEOLDniaBdQ1tk-tgoNuL5lhZNjQUe648ZgRuUWrLaIX3oHWCh15jyNqf9dfjjRB70aXgFCI2nC397dI72pLnPhLIqqOKxPTEV4El04FFBbjsdOPDp3q0LQurvyiZmQB21o9E3WEoCcol53Fx2g0SIc5ixunqT-a4RIeTz8CzvY7DdPxvqKSbekvmaTBa_UWB3gbtD9Y%253D&ctt=1675447519123'\n",
    "\n",
    "    if start_n_page == None :\n",
    "        start_n_page = 0\n",
    "    if end_n_page == None :\n",
    "        end_n_page = len (links)\n",
    "\n",
    "    print ('link to scrape: ' , end_n_page - start_n_page)\n",
    "\n",
    "    for base_url in links [start_n_page :end_n_page] :\n",
    "\n",
    "        n_link += 1\n",
    "        driver = webdriver.Chrome (r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\")\n",
    "        driver.get (base_url)\n",
    "        a = np.random.choice ([x for x in range (3 , 5)])\n",
    "        time_sleep = time_sleep + a\n",
    "        time.sleep (a)\n",
    "        ## cookies\n",
    "        allow_cookies (driver)\n",
    "        a = np.random.choice ([x for x in range (3 , 5)])\n",
    "        time_sleep = time_sleep + a\n",
    "        time.sleep (a)\n",
    "        ## Mapping dei bottoni\n",
    "        general_button = driver.find_elements (By.CSS_SELECTOR , \".css-dkrzi8.e1eh6fgm0\")\n",
    "        button_mapping = {valore.text : indice for indice , valore in enumerate (general_button)}\n",
    "\n",
    "        d_valutazione = {}\n",
    "        d_azienda = {}\n",
    "\n",
    "        print ('check the buttons-->' , n_link)\n",
    "        try :\n",
    "            general_button = driver.find_elements (By.CSS_SELECTOR , \".css-dkrzi8.e1eh6fgm0\")\n",
    "\n",
    "        except :\n",
    "            print ('no buttons found')\n",
    "            general_button = []\n",
    "\n",
    "        if len (general_button) > 0 :\n",
    "\n",
    "            try :\n",
    "\n",
    "                azienda = general_button [button_mapping ['Azienda']]  # Azienda\n",
    "                azienda.click ()\n",
    "                a = np.random.choice ([x for x in range (3 , 5)])\n",
    "                time_sleep = time_sleep + a\n",
    "                time.sleep (a)\n",
    "                pag_azienda_span = driver.find_elements (By.CSS_SELECTOR , \".css-vugejy.es5l5kg0 span.value\")\n",
    "                pag_azienda_value = driver.find_elements (By.CSS_SELECTOR , \".css-vugejy.es5l5kg0 label\")\n",
    "\n",
    "                azienda_span = []\n",
    "                azienda_value = []\n",
    "\n",
    "                for e in pag_azienda_span :\n",
    "                    azienda_span.append (e.text)\n",
    "\n",
    "                for e in pag_azienda_value :\n",
    "                    azienda_value.append (e.text)\n",
    "                d_azienda = dict (zip (azienda_value , azienda_span))\n",
    "                print ('scraped company page')\n",
    "\n",
    "\n",
    "            except :\n",
    "                pass\n",
    "\n",
    "            try :\n",
    "                valutazione = general_button [button_mapping ['Valutazione']]  # Valutazione\n",
    "                # recensioni = general_button[2] # Recensioni\n",
    "\n",
    "                ## Cambio foglio\n",
    "                valutazione.click ()\n",
    "                pag_valutazione = driver.find_elements (By.CSS_SELECTOR , \".css-a7hxlj.e121l59f1\")\n",
    "                a = np.random.choice ([x for x in range (3 , 5)])\n",
    "                time_sleep = time_sleep + a\n",
    "                time.sleep (a)\n",
    "                valutazione_1 = [pag_valutazione [0].text , pag_valutazione [2].text , pag_valutazione [4].text ,\n",
    "                                 pag_valutazione [6].text , pag_valutazione [8].text]\n",
    "                valutazione_2 = [pag_valutazione [1].text , pag_valutazione [3].text , pag_valutazione [5].text ,\n",
    "                                 pag_valutazione [7].text , pag_valutazione [9].text]\n",
    "                print ('scraped evaluation page')\n",
    "\n",
    "                d_valutazione = dict (zip (valutazione_1 , valutazione_2))\n",
    "\n",
    "            except :\n",
    "                pass  # to add benefit e stipendio\n",
    "\n",
    "        d = {**d_valutazione , **d_azienda}\n",
    "\n",
    "        driver.quit ()\n",
    "\n",
    "        df2 = pd.DataFrame.from_dict (d , orient = 'index').T\n",
    "        df2 ['job_link'] = base_url\n",
    "\n",
    "        df_append_2 = df_append_2.append (df2).reset_index (drop = True)\n",
    "\n",
    "    end_time = time.time ()\n",
    "    print ('Total pages scraped:' , len (links) , '\\n')\n",
    "    print (\"Runtime:\" , round (end_time - start_time) , \"seconds\" + '\\nTime sleep: ' , time_sleep , 'seconds')\n",
    "    df_append_2 = df_append_2.rename(columns={'Sede centrale':'sede_centrale',\n",
    "                                    'Dimensioni':'dimensioni',\n",
    "                                    'Fondata nel' : 'fondata_nel',\n",
    "                                    'Tipo':'tipo',\n",
    "                                    'Entrate':'entrate',\n",
    "                                    # job_link\n",
    "                                    'Opportunità di carriera': 'oppurtunita_carriera',\n",
    "                                    'Stipendio e benefit':'stipendio_e_benefit',\n",
    "                                    'Cultura e valori':'cultura_e_valori',\n",
    "                                    'Dirigenti senior':'dirigenti_senior',\n",
    "                                    'Equilibrio lavoro/vita privata':'equilibrio_lavoro_vita_privata',\n",
    "                                    'Settore':'settore',\n",
    "                                    'Segmento':'segmento'})\n",
    "\n",
    "    return df_append_2\n",
    "\n",
    "\n",
    "#sys.path.insert(0, '../funzioni')\n",
    "#from funzioni_ingestion import *\n",
    "\n",
    "def check_and_scrape_company_pages(df_company_overview, df, file_path):\n",
    "    old_links = list(df_company_overview['job_link'].unique())\n",
    "    links = list(df['job_link'].unique())\n",
    "    new_links = list(set(links) - set(old_links))\n",
    "    print('Pagine su cui fare scraping', len(new_links))\n",
    "    df_scraping_company=pd.DataFrame()\n",
    "    n=1\n",
    "    for i in range(0, len(new_links), n):\n",
    "        df_fitt = scraping_company_page(new_links, end_n_page = i+n, start_n_page=i)\n",
    "        if len(df_fitt)==0:\n",
    "            columns = ['job_link', 'oppurtunita_carriera', 'stipendio_e_benefit', 'cultura_e_valori', \n",
    "                       'dirigenti_senior', 'equilibrio_lavoro_vita_privata', 'sede_centrale', 'dimensioni',\n",
    "                       'tipo', 'entrate', 'fondata_nel', 'settore', 'segmento']\n",
    "            \n",
    "            values = [new_links[i]] + ['Non_disponibile'] * (len(columns) - 1)\n",
    "            \n",
    "            df_fitt = pd.DataFrame([values], columns=columns)\n",
    "            print(len(df_fitt))\n",
    "            if len(df_scraping_company) == np.random.choice ([x for x in range (150 , 450)]):\n",
    "                print(len(df_scraping_company))\n",
    "                raise ValueError(\"Stop\")\n",
    "            \n",
    "        df_scraping_company = df_scraping_company.append(df_fitt).reset_index(drop=True)\n",
    "        df_company_overview.append(df_scraping_company).reset_index(drop=True).fillna('Non_disponibile').to_csv('../data/company_overview.csv', index=False)\n",
    "        print('Lista parziale', len(df_scraping_company),'Lista totale', len(pd.read_csv('../data/company_overview.csv')))\n",
    "    df_final = df.merge(df_scraping_company, how='left', on ='job_link').drop_duplicates().reset_index(drop=True)\n",
    "    df_final.to_csv(file_path, index=False)\n",
    "\n",
    "def run_scraping(url=None, file_esclusi=[], includi_solo=[]):\n",
    "    path_overview = '../data/company_overview.csv'\n",
    "    path = '../data'\n",
    "    today = date.today()\n",
    "\n",
    "    data = today.strftime(\"%Y%m%d\")\n",
    "    data_file = today.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    if url == None:\n",
    "        print('Ricerco solo schede di valutazione')\n",
    "        df_company_overview = pd.read_csv(path_overview)\n",
    "        file_list = os.listdir(path)\n",
    "        file_da_escludere = ['company_overview.csv', 'jobs']\n",
    "        file_list = list(set(file_list) - set(file_da_escludere) - set(file_esclusi))\n",
    "        if len(includi_solo)>0:\n",
    "            file_list = includi_solo\n",
    "        for e in file_list:\n",
    "            df_partial_company=pd.DataFrame()\n",
    "            file_path = path + '/' + e\n",
    "            print(file_path)\n",
    "            df = pd.read_csv(file_path).drop_duplicates().reset_index(drop=True)\n",
    "            check_and_scrape_company_pages(df_company_overview, df, file_path)\n",
    "            print('Ho completato il file nel path', file_path)\n",
    "    else:\n",
    "        print('Faccio lo scraping del link che mi hai chiesto e poi ricerco le schede di valutazione')\n",
    "        df_job_scraping = scraping_job_page(url, n_page=450)\n",
    "        df_company_overview = pd.read_csv(path_overview)\n",
    "        df_job_scraping['scraping_date'] = data_file\n",
    "        file_path=path +\"/jobs/scraping_job_\" + data + '.csv'\n",
    "        df_job_scraping.to_csv(file_path, index=False)\n",
    "        check_and_scrape_company_pages(df_company_overview, df_job_scraping, file_path)\n",
    "        \n",
    "def parsing_data():\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    path = '../data'\n",
    "    \n",
    "    float_columns = [\n",
    "            'company_rating','oppurtunita_carriera', 'stipendio_e_benefit',\n",
    "           'cultura_e_valori', 'dirigenti_senior',\n",
    "           'equilibrio_lavoro_vita_privata']\n",
    "    \n",
    "    string_columns = [\n",
    "        'company', 'job_title', 'location', 'job_age',\n",
    "           'job_link', 'sede_centrale', 'dimensioni', 'tipo', 'settore', 'segmento', 'entrate']\n",
    "    \n",
    "    integer_columns = ['fondata_nel']\n",
    "    \n",
    "    dfaaa = pd.read_csv('../data/company_overview.csv')\n",
    "    \n",
    "    file_list = os.listdir(path)\n",
    "    file_da_escludere = ['company_overview.csv', 'jobs']\n",
    "    \n",
    "    file_list = list(set(file_list) - set(file_da_escludere))\n",
    "    for e in file_list:\n",
    "        file_path = path + '/' + e\n",
    "        df = pd.read_csv(file_path).drop_duplicates().reset_index(drop=True)\n",
    "        df = df[['company', 'job_title', 'location', 'company_rating', 'job_age', 'job_link']]\n",
    "        df = df.merge(dfaaa, how='left', on='job_link')\n",
    "        #file_path[-12:-4]\n",
    "        a = pd.to_datetime(file_path[-12:-4], format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "        \n",
    "        df['scraping_date'] = a\n",
    "        try:\n",
    "            df['company_rating'] = df['company_rating'].str.extract(r'>\\s*([0-9.]+)\\s*<')\n",
    "        except:\n",
    "            pass\n",
    "        for e in float_columns:\n",
    "            df[e] = df[e].replace('Non_disponibile',0).astype(float)\n",
    "        \n",
    "        for e in string_columns:\n",
    "            df[e] = df[e].astype(str)\n",
    "            \n",
    "        for e in integer_columns:\n",
    "            df[e] = df[e].fillna(0).replace('Non_disponibile',0).astype(float).astype(int)\n",
    "            \n",
    "        df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b4d7a",
   "metadata": {},
   "source": [
    "La funzione \"run_scraping\" completa entrambi gli step indicati:\n",
    "1. scraping delle offerte di lavoro,\n",
    "2. scraping della scheda di valutazione delle aziende che offrono un lavoro.\n",
    "\n",
    "È possibile non compiere il punto 1 e utilizzare soltanto il punto 2. In questo caso verranno cercati tutti i job_link non presenti nei dati raccolti e verranno scrapate soltanto le scheda di valutazione delle aziende che offrono un lavoro.  \n",
    "Questa scelta è stata dettata dalla differenti performance dei due processi, il primo molto veloce il secondo meno.  \n",
    "In questo modo è possibile completare le offerte di lavoro con la scheda valutativa dell'azienda.  \n",
    "\n",
    "Per non effettuare il punto 1 sarà necessario non specificare l'url. Se si vuole invece completare la scheda valutativa di un determinato file sarà necessario inserire il nome del file nell'argomento \"includi_solo\".  \n",
    "Se si vuole invece completare la scheda valutativa di tutti i file al di fuori di uno o più sarà necessario inserire il nome/i nell'argomento \"file_esclusi\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.glassdoor.it/Lavoro/bari-lavori-SRCH_IL.0,4_IC2835879.htm'\n",
    "run_scraping(url='https://www.glassdoor.it/Lavoro/milano-lavori-SRCH_IL.0,6_IC2802090.htm', file_esclusi=[], includi_solo=[])\n",
    "parsing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cb4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589be14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
