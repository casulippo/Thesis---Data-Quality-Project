{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install getuseragent\n",
    "# ! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a268a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "# BeautifulSoup e Request\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from getuseragent import UserAgent\n",
    "\n",
    "# selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be91df",
   "metadata": {},
   "source": [
    "La data ingestion è un processo che consiste nell'ottenere e importare dati, per utilizzo immediato o l'archiviazione in un database o altri sistemi di gestione dei dati.\n",
    "\n",
    "L'obiettivo è quello di collezionare le informazioni delle offerte di lavoro nel modo più dettagliato possibile sito www.glassdoor.com ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb9d6a",
   "metadata": {},
   "source": [
    "### Definizione delle funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "useragent = UserAgent()\n",
    "\n",
    "theuseragent = useragent.Random()\n",
    "headers = {'User-Agent': theuseragent}\n",
    "header = {\n",
    "    \"user-agent\": theuseragent ,\n",
    "    'referer':'https://www.google.com/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5684ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allow_cookies(driver):    \n",
    "    button_setting_cookies = driver.find_element(By.CSS_SELECTOR, \"button.cookie-setting-link\")\n",
    "    button_setting_cookies.click()\n",
    "    time.sleep(np.random.choice([x/10 for x in range(7,22)]))    \n",
    "    button_confirm_cookies = driver.find_element(By.CSS_SELECTOR, \"button.save-preference-btn-handler\")\n",
    "    #Conferma le mie scelte\n",
    "    button_confirm_cookies.click()\n",
    "\n",
    "def next_page():\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"button.nextButton\")\n",
    "    next_button.click()\n",
    "    url = driver.current_url\n",
    "\n",
    "def parse_url(url):\n",
    "    company_name = []\n",
    "    job_title=[]\n",
    "    location=[]\n",
    "    company_rating=[]\n",
    "    job_age=[]\n",
    "    job_link=[]\n",
    "    \n",
    "    r = requests.get(url, headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    job_search_div = soup.select('div#JobSearch')[0]\n",
    "    a = job_search_div.select('div#PageBodyContents')\n",
    "    b = a[0].select('div#JobResults')\n",
    "    c = b[0].select('article#MainCol')\n",
    "    len_li = len(c[0].find_all('li', class_='react-job-listing'))\n",
    "    lis = c[0].find_all('li', class_='react-job-listing')\n",
    "    \n",
    "    len_li = len(lis)\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for e in lis[0:len_li]:\n",
    "        try:\n",
    "            company_name.append(e.find('div').find('a')['title'])\n",
    "        except:\n",
    "            company_name.append(None)\n",
    "        try:\n",
    "            job_title.append(e['data-normalize-job-title'])\n",
    "        except:\n",
    "            job_title.append(None)    \n",
    "        try:\n",
    "            location.append(e['data-job-loc'])\n",
    "        except:\n",
    "            location.append(None)\n",
    "        try:\n",
    "            company_rating.append(e.find('span', class_='css-2lqh28')) ### da controllare\n",
    "        except:\n",
    "            company_rating.append(None)\n",
    "        try:\n",
    "            job_age.append(e.find(\"div\", {\"data-test\": \"job-age\"}).text)\n",
    "        except:\n",
    "            job_link.append(None)\n",
    "        try:\n",
    "            link = \"https://www.glassdoor.it\" + e.find(\"a\", {\"data-test\": \"job-link\"}).get(\"href\")\n",
    "            job_link.append(link)\n",
    "        except:\n",
    "            job_link.append(None)\n",
    "            \n",
    "    df['company'] = company_name\n",
    "    df['job_title'] = job_title\n",
    "    df['location'] = location\n",
    "    df['company_rating']=company_rating\n",
    "    df['job_age'] = job_age\n",
    "    df['job_link']=job_link\n",
    "    \n",
    "    return df\n",
    "            \n",
    "def scraping_job_page(base_url):\n",
    "    df_append = pd.DataFrame()\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\") ## inserire il path del proprio chromedriver\n",
    "    # base_url = 'https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0'\n",
    "    driver.get(base_url)\n",
    "    a = np.random.choice([x for x in range(7,15)])\n",
    "    time.sleep(a)\n",
    "    url = driver.current_url\n",
    "    allow_cookies(driver)\n",
    "    a = np.random.choice([x for x in range(7,15)])\n",
    "    time.sleep(a)\n",
    "    r = requests.get(url, headers=header)\n",
    "    \n",
    "    text = BeautifulSoup(r.text, 'html.parser').find('div', attrs={'class': 'paginationFooter'}).text\n",
    "    \n",
    "    int_list=[]\n",
    "    for e in re.findall(r'-?\\d+\\.?\\d*', text):\n",
    "        int_list.append(int(e))\n",
    "    \n",
    "    n_page = max(int_list)\n",
    "    #n_page = 5\n",
    "    print('Numero di pagine sui cui fare scraping: ' + str(n_page))\n",
    "    a = np.random.choice([x for x in range(7,15)])\n",
    "    time.sleep(a)\n",
    "    \n",
    "    for page in range(n_page):\n",
    "        print(str(page +1) + '/' + str(n_page))\n",
    "        a = np.random.choice([x for x in range(7,15)])\n",
    "        time.sleep(a)\n",
    "        next_page()\n",
    "        url = driver.current_url\n",
    "        if page == 0:\n",
    "            time.sleep(5)\n",
    "            close_button = driver.find_element(By.CSS_SELECTOR, \"svg.SVGInline-svg.modal_closeIcon-svg\")\n",
    "            close_button.click()\n",
    "        df = parse_url(url)\n",
    "        df_append = df_append.append(df).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "        \n",
    "    return df_append\n",
    "\n",
    "\n",
    "def scraping_company_page(links):\n",
    "    start_time = time.time()\n",
    "    df_append_2 =pd.DataFrame()\n",
    "    time_sleep = 0\n",
    "    n_link = 0\n",
    "    ## lista dei link\n",
    "    \n",
    "    #base_url = 'https://www.glassdoor.it/job-listing/field-service-engineer-technician-lexas-JV_IC2802269_KO0,33_KE34,39.htm?jl=1008135983566&pos=101&ao=1110586&s=58&guid=000001861873f6d389d965e51ce717e2&src=GD_JOB_AD&t=SR&vt=w&uido=FEB79CBEB143B609D33C645CD13E2F4B&ea=1&cs=1_524e761d&cb=1675447498725&jobListingId=1008135983566&cpc=65CC663E25211861&jrtk=3-0-1goc77to1g2ev801-1goc77tp0i6id800-7cbed71e05432cdf--6NYlbfkN0CVO0F7mWis5ReNIXvK0Cy97GKSpj_H8mHyNoiV7tLwhxrGQFeFbXfrFFwDAnfvPXeiJe5SavTtAEQpKcpYVReYHZsV-4ZX7UeAkoBb0f_WCVWviQdPDhB0WcxVHddsJTu6CPWu9hRPncXvGLdy3ZffF5b3aOd7vp19QcNQdw0qQd1bkijbQHvL2CZX_Cxp4BGS1Sk8JgAjiz75HrAHRuR5hA9kjnxafzWfGAAOJBSKybBbJtFcKCvWC2Py0-IgF36KHcIY5QbFzm8TqI0WJJ75VyN8D93fcG7Ikeu9ECT-vBbPKtsVv7AOpI6elTa0KTJqDzeYeI39a8bHa5BUUOjrQ1rsJFwMGLJrLrbqXOIObHs1pSu_fpr-FcNbNcmvwdLQufgm_hOEka1AR5pS2jw3Kd3MEOLDniaBdQ1tk-tgoNuL5lhZNjQUe648ZgRuUWrLaIX3oHWCh15jyNqf9dfjjRB70aXgFCI2nC397dI72pLnPhLIqqOKxPTEV4El04FFBbjsdOPDp3q0LQurvyiZmQB21o9E3WEoCcol53Fx2g0SIc5ixunqT-a4RIeTz8CzvY7DdPxvqKSbekvmaTBa_UWB3gbtD9Y%253D&ctt=1675447519123'\n",
    "    print('link to scrape: ', len(links))    \n",
    "    \n",
    "    for base_url in links: \n",
    "        n_link += 1\n",
    "        driver = webdriver.Chrome(r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\")\n",
    "        driver.get(base_url)\n",
    "        a = np.random.choice([x for x in range(7,15)])\n",
    "        time_sleep = time_sleep + a \n",
    "        time.sleep(a)\n",
    "        ## cookies\n",
    "        allow_cookies(driver)\n",
    "        a = np.random.choice([x for x in range(7,15)])\n",
    "        time_sleep = time_sleep + a \n",
    "        time.sleep(a)\n",
    "        ## Mapping dei bottoni\n",
    "        general_button = driver.find_elements(By.CSS_SELECTOR, \".css-dkrzi8.e1eh6fgm0\")\n",
    "        button_mapping = {valore.text: indice for indice, valore in enumerate(general_button)}\n",
    "    \n",
    "        \n",
    "        d_valutazione={}\n",
    "        d_azienda={}\n",
    "    \n",
    "        print('check the buttons-->', n_link)\n",
    "        try:\n",
    "            general_button = driver.find_elements(By.CSS_SELECTOR, \".css-dkrzi8.e1eh6fgm0\")\n",
    "            \n",
    "        except:\n",
    "            print('no buttons found')\n",
    "            general_button = []\n",
    "        \n",
    "        \n",
    "        if len(general_button)>0:\n",
    "            \n",
    "            try:\n",
    "    \n",
    "                azienda = general_button[button_mapping['Azienda']] # Azienda\n",
    "                azienda.click()\n",
    "                pag_azienda_span = driver.find_elements(By.CSS_SELECTOR, \".css-vugejy.es5l5kg0 span.value\")\n",
    "                pag_azienda_value = driver.find_elements(By.CSS_SELECTOR, \".css-vugejy.es5l5kg0 label\")\n",
    "                \n",
    "                azienda_span = []\n",
    "                azienda_value = []\n",
    "                \n",
    "                for e in pag_azienda_span:\n",
    "                    azienda_span.append(e.text)\n",
    "                \n",
    "                for e in pag_azienda_value:\n",
    "                    azienda_value.append(e.text)\n",
    "                d_azienda = dict(zip(azienda_value, azienda_span))\n",
    "            \n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                valutazione = general_button[button_mapping['Valutazione']] # Valutazione\n",
    "                #recensioni = general_button[2] # Recensioni\n",
    "                \n",
    "                ## Cambio foglio \n",
    "                valutazione.click()\n",
    "                pag_valutazione = driver.find_elements(By.CSS_SELECTOR, \".css-a7hxlj.e121l59f1\")\n",
    "                a = np.random.choice([x for x in range(7,15)])\n",
    "                time_sleep = time_sleep + a \n",
    "                time.sleep(a)\n",
    "                valutazione_1 = [pag_valutazione[0].text, pag_valutazione[2].text, pag_valutazione[4].text,pag_valutazione[6].text, pag_valutazione[8].text]\n",
    "                valutazione_2 = [pag_valutazione[1].text, pag_valutazione[3].text, pag_valutazione[5].text,pag_valutazione[7].text, pag_valutazione[9].text]\n",
    "                print('scraped evaluation page')\n",
    "    \n",
    "                d_valutazione = dict(zip(valutazione_1, valutazione_2))\n",
    "            \n",
    "            except:\n",
    "                pass  \n",
    "            # to add benefit e stipendio\n",
    "    \n",
    "                \n",
    "            \n",
    "        d = {**d_valutazione, **d_azienda}\n",
    "            \n",
    "        driver.quit()\n",
    "        \n",
    "        \n",
    "        df2 = pd.DataFrame.from_dict(d,  orient='index').T\n",
    "        df2['job_link']=base_url\n",
    "        \n",
    "        df_append_2 = df_append_2.append(df2).reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Total pages scraped:', len(links), '\\n')\n",
    "    print(\"Runtime:\", round(end_time - start_time), \"seconds\" + '\\nTime sleep: ', time_sleep, 'seconds')\n",
    "    return df_append_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830952e",
   "metadata": {},
   "source": [
    "Una volta definite le funzioni per procedere con lo scraping, ci saranno due faso di scraping:\n",
    "1. Scraping delle offerte di lavoro \n",
    "2. Scraping delle aziende che offrono un lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86d661",
   "metadata": {},
   "source": [
    "### Scraping delle offerte di lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cff3a8",
   "metadata": {},
   "source": [
    "Questa prima fase si occuperà di fare scraping sulle offerte di lavoro relative ad un determinato link.\n",
    "(es [offerte lavoro amsterdam]('https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0'))\n",
    "\n",
    "\n",
    "Se il link presenta più pagine relative alle offerte cercate, il sistema andrà a fare lo scraping su tutte le pagine sino ad arrivare alla 30°.\n",
    "\n",
    "Dopo aver avviato la navigazione web il sistema rifiuterà i cookies e inizierà lo scraping per tutte le pagine presenti nel link.  \n",
    "Una volta completato lo scraping verrà creato un dataframe con le informazioni raccolte. Il dataframe presenterà queste colonne:\n",
    "- company: nome dell'azienda\n",
    "- job_title: Posizione lavorativa offerta\n",
    "- location: Sede dell'offerta di lavoro\n",
    "- company_rating: rating dell'azienda (secondo gli utenti di Glassdoor)\n",
    "- job_age: giorni passati dalla pubblicazione dell'offerta\n",
    "- job_link: link dell'azienda che ha pubblicato il lavoro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643bf52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
