{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install getuseragent\n",
    "# ! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a268a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "from datetime import date\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "# BeautifulSoup e Request\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from getuseragent import UserAgent\n",
    "\n",
    "# selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.glassdoor.it/Lavoro/rende-italia-lavori-SRCH_IL.0,12_IC2765306.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Rende%2520(Italia)&context=Jobs&dropdown=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= 'scraping_all_amsterdam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "\n",
    "data = today.strftime(\"%Y%m%d\")\n",
    "data_file = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "data, data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be91df",
   "metadata": {},
   "source": [
    "La data ingestion è un processo che consiste nell'ottenere e importare dati, per utilizzo immediato o l'archiviazione in un database o altri sistemi di gestione dei dati.\n",
    "\n",
    "L'obiettivo è quello di collezionare le informazioni delle offerte di lavoro nel modo più dettagliato possibile sito www.glassdoor.com ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb9d6a",
   "metadata": {},
   "source": [
    "### Definizione delle funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "useragent = UserAgent()\n",
    "\n",
    "theuseragent = useragent.Random()\n",
    "headers = {'User-Agent': theuseragent}\n",
    "header = {\n",
    "    \"user-agent\": theuseragent ,\n",
    "    'referer':'https://www.google.com/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5684ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allow_cookies(driver):\n",
    "    print('Decline Cookies')\n",
    "    button_setting_cookies = driver.find_element(By.CSS_SELECTOR, \"button.cookie-setting-link\")\n",
    "    button_setting_cookies.click()\n",
    "    time.sleep(np.random.choice([x/10 for x in range(7,22)]))    \n",
    "    button_confirm_cookies = driver.find_element(By.CSS_SELECTOR, \"button.save-preference-btn-handler\")\n",
    "    #Conferma le mie scelte\n",
    "    button_confirm_cookies.click()\n",
    "\n",
    "def next_page(driver):\n",
    "    print('Changing page')\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"button.nextButton\")\n",
    "    next_button.click()\n",
    "    url = driver.current_url\n",
    "\n",
    "def parse_url(url):\n",
    "    company_name = []\n",
    "    job_title=[]\n",
    "    location=[]\n",
    "    company_rating=[]\n",
    "    job_age=[]\n",
    "    job_link=[]\n",
    "    \n",
    "    r = requests.get(url, headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    job_search_div = soup.select('div#JobSearch')[0]\n",
    "    a = job_search_div.select('div#PageBodyContents')\n",
    "    b = a[0].select('div#JobResults')\n",
    "    c = b[0].select('article#MainCol')\n",
    "    len_li = len(c[0].find_all('li', class_='react-job-listing'))\n",
    "    lis = c[0].find_all('li', class_='react-job-listing')\n",
    "    \n",
    "    len_li = len(lis)\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for e in lis[0:len_li]:\n",
    "        try:\n",
    "            company_name.append(e.find('div').find('a')['title'])\n",
    "        except:\n",
    "            company_name.append(None)\n",
    "        try:\n",
    "            job_title.append(e['data-normalize-job-title'])\n",
    "        except:\n",
    "            job_title.append(None)    \n",
    "        try:\n",
    "            location.append(e['data-job-loc'])\n",
    "        except:\n",
    "            location.append(None)\n",
    "        try:\n",
    "            company_rating.append(e.find('span', class_='css-2lqh28')) ### da controllare\n",
    "        except:\n",
    "            company_rating.append(None)\n",
    "        try:\n",
    "            job_age.append(e.find(\"div\", {\"data-test\": \"job-age\"}).text)\n",
    "        except:\n",
    "            job_link.append(None)\n",
    "        try:\n",
    "            link = \"https://www.glassdoor.it\" + e.find(\"a\", {\"data-test\": \"job-link\"}).get(\"href\")\n",
    "            job_link.append(link)\n",
    "        except:\n",
    "            job_link.append(None)\n",
    "            \n",
    "    df['company'] = company_name\n",
    "    df['job_title'] = job_title\n",
    "    df['location'] = location\n",
    "    df['company_rating']=company_rating\n",
    "    df['job_age'] = job_age\n",
    "    df['job_link']=job_link\n",
    "    \n",
    "    return df\n",
    "            \n",
    "def scraping_job_page(base_url, n_page=None):\n",
    "    start_time = time.time()\n",
    "    time_sleep = 0\n",
    "    df_append = pd.DataFrame()\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\")\n",
    "    # base_url = 'https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0'\n",
    "    driver.get(base_url)\n",
    "    a = np.random.choice([x for x in range(7,15)])\n",
    "    time_sleep = time_sleep + a \n",
    "    time.sleep(a)\n",
    "    url = driver.current_url\n",
    "    #driver.maximize_window()\n",
    "    allow_cookies(driver)\n",
    "    a = np.random.choice([x for x in range(7,15)])\n",
    "    time_sleep = time_sleep + a \n",
    "    time.sleep(a)\n",
    "    r = requests.get(url, headers=header)\n",
    "    \n",
    "    text = BeautifulSoup(r.text, 'html.parser').find('div', attrs={'class': 'paginationFooter'}).text\n",
    "    \n",
    "    int_list=[]\n",
    "    for e in re.findall(r'-?\\d+\\.?\\d*', text):\n",
    "        int_list.append(int(e))\n",
    "        \n",
    "    if n_page == None:\n",
    "        n_page = max(int_list)\n",
    "    #n_page = 5\n",
    "    print('Numero di pagine sui cui fare scraping: ' + str(n_page))\n",
    "    a = np.random.choice([x for x in range(7,15)])\n",
    "    time_sleep = time_sleep + a \n",
    "    time.sleep(a)\n",
    "    \n",
    "    for page in range(n_page):\n",
    "        print(str(page +1) + '/' + str(n_page))\n",
    "        a = np.random.choice([x for x in range(7,15)])\n",
    "        time_sleep = time_sleep + a \n",
    "        time.sleep(a)\n",
    "        next_page(driver)\n",
    "        url = driver.current_url\n",
    "        if page == 0:\n",
    "            time.sleep(5)\n",
    "            close_button = driver.find_element(By.CSS_SELECTOR, \"svg.SVGInline-svg.modal_closeIcon-svg\")\n",
    "            close_button.click()\n",
    "        df = parse_url(url)\n",
    "        df_append = df_append.append(df).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "    end_time = time.time()\n",
    "    print('Done!')\n",
    "    print('Total pages scraped:', n_page, '\\n')\n",
    "    print(\"Runtime:\", round(end_time - start_time), \"seconds\" + '\\nTime sleep:', time_sleep, 'seconds')\n",
    "        \n",
    "    return df_append\n",
    "\n",
    "\n",
    "def scraping_company_page(links, end_n_page=None, start_n_page=None):\n",
    "    start_time = time.time()\n",
    "    df_append_2 =pd.DataFrame()\n",
    "    time_sleep = 0\n",
    "    n_link = 0\n",
    "    ## lista dei link\n",
    "    \n",
    "    #base_url = 'https://www.glassdoor.it/job-listing/field-service-engineer-technician-lexas-JV_IC2802269_KO0,33_KE34,39.htm?jl=1008135983566&pos=101&ao=1110586&s=58&guid=000001861873f6d389d965e51ce717e2&src=GD_JOB_AD&t=SR&vt=w&uido=FEB79CBEB143B609D33C645CD13E2F4B&ea=1&cs=1_524e761d&cb=1675447498725&jobListingId=1008135983566&cpc=65CC663E25211861&jrtk=3-0-1goc77to1g2ev801-1goc77tp0i6id800-7cbed71e05432cdf--6NYlbfkN0CVO0F7mWis5ReNIXvK0Cy97GKSpj_H8mHyNoiV7tLwhxrGQFeFbXfrFFwDAnfvPXeiJe5SavTtAEQpKcpYVReYHZsV-4ZX7UeAkoBb0f_WCVWviQdPDhB0WcxVHddsJTu6CPWu9hRPncXvGLdy3ZffF5b3aOd7vp19QcNQdw0qQd1bkijbQHvL2CZX_Cxp4BGS1Sk8JgAjiz75HrAHRuR5hA9kjnxafzWfGAAOJBSKybBbJtFcKCvWC2Py0-IgF36KHcIY5QbFzm8TqI0WJJ75VyN8D93fcG7Ikeu9ECT-vBbPKtsVv7AOpI6elTa0KTJqDzeYeI39a8bHa5BUUOjrQ1rsJFwMGLJrLrbqXOIObHs1pSu_fpr-FcNbNcmvwdLQufgm_hOEka1AR5pS2jw3Kd3MEOLDniaBdQ1tk-tgoNuL5lhZNjQUe648ZgRuUWrLaIX3oHWCh15jyNqf9dfjjRB70aXgFCI2nC397dI72pLnPhLIqqOKxPTEV4El04FFBbjsdOPDp3q0LQurvyiZmQB21o9E3WEoCcol53Fx2g0SIc5ixunqT-a4RIeTz8CzvY7DdPxvqKSbekvmaTBa_UWB3gbtD9Y%253D&ctt=1675447519123'\n",
    "       \n",
    "    \n",
    "    if start_n_page == None:\n",
    "        start_n_page=0\n",
    "    if end_n_page == None:\n",
    "        end_n_page=len(links)\n",
    "        \n",
    "    print('link to scrape: ', end_n_page - start_n_page) \n",
    "    \n",
    "    for base_url in links[start_n_page:end_n_page]: \n",
    "        n_link += 1\n",
    "        driver = webdriver.Chrome(r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\")\n",
    "        driver.get(base_url)\n",
    "        a = np.random.choice([x for x in range(7,15)])\n",
    "        time_sleep = time_sleep + a \n",
    "        time.sleep(a)\n",
    "        ## cookies\n",
    "        allow_cookies(driver)\n",
    "        a = np.random.choice([x for x in range(7,15)])\n",
    "        time_sleep = time_sleep + a \n",
    "        time.sleep(a)\n",
    "        ## Mapping dei bottoni\n",
    "        general_button = driver.find_elements(By.CSS_SELECTOR, \".css-dkrzi8.e1eh6fgm0\")\n",
    "        button_mapping = {valore.text: indice for indice, valore in enumerate(general_button)}\n",
    "    \n",
    "        \n",
    "        d_valutazione={}\n",
    "        d_azienda={}\n",
    "    \n",
    "        print('check the buttons-->', n_link)\n",
    "        try:\n",
    "            general_button = driver.find_elements(By.CSS_SELECTOR, \".css-dkrzi8.e1eh6fgm0\")\n",
    "            \n",
    "        except:\n",
    "            print('no buttons found')\n",
    "            general_button = []\n",
    "        \n",
    "        \n",
    "        if len(general_button)>0:\n",
    "            \n",
    "            try:\n",
    "    \n",
    "                azienda = general_button[button_mapping['Azienda']] # Azienda\n",
    "                azienda.click()\n",
    "                a = np.random.choice([x for x in range(7,15)])\n",
    "                time_sleep = time_sleep + a \n",
    "                time.sleep(a)\n",
    "                pag_azienda_span = driver.find_elements(By.CSS_SELECTOR, \".css-vugejy.es5l5kg0 span.value\")\n",
    "                pag_azienda_value = driver.find_elements(By.CSS_SELECTOR, \".css-vugejy.es5l5kg0 label\")\n",
    "                \n",
    "                azienda_span = []\n",
    "                azienda_value = []\n",
    "                \n",
    "                for e in pag_azienda_span:\n",
    "                    azienda_span.append(e.text)\n",
    "                \n",
    "                for e in pag_azienda_value:\n",
    "                    azienda_value.append(e.text)\n",
    "                d_azienda = dict(zip(azienda_value, azienda_span))\n",
    "                print('scraped company page')\n",
    "            \n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                valutazione = general_button[button_mapping['Valutazione']] # Valutazione\n",
    "                #recensioni = general_button[2] # Recensioni\n",
    "                \n",
    "                ## Cambio foglio \n",
    "                valutazione.click()\n",
    "                pag_valutazione = driver.find_elements(By.CSS_SELECTOR, \".css-a7hxlj.e121l59f1\")\n",
    "                a = np.random.choice([x for x in range(7,15)])\n",
    "                time_sleep = time_sleep + a \n",
    "                time.sleep(a)\n",
    "                valutazione_1 = [pag_valutazione[0].text, pag_valutazione[2].text, pag_valutazione[4].text,pag_valutazione[6].text, pag_valutazione[8].text]\n",
    "                valutazione_2 = [pag_valutazione[1].text, pag_valutazione[3].text, pag_valutazione[5].text,pag_valutazione[7].text, pag_valutazione[9].text]\n",
    "                print('scraped evaluation page')\n",
    "    \n",
    "                d_valutazione = dict(zip(valutazione_1, valutazione_2))\n",
    "            \n",
    "            except:\n",
    "                pass  \n",
    "            # to add benefit e stipendio\n",
    "    \n",
    "                \n",
    "            \n",
    "        d = {**d_valutazione, **d_azienda}\n",
    "            \n",
    "        driver.quit()\n",
    "        \n",
    "        \n",
    "        df2 = pd.DataFrame.from_dict(d,  orient='index').T\n",
    "        df2['job_link']=base_url\n",
    "        \n",
    "        df_append_2 = df_append_2.append(df2).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Total pages scraped:', len(links), '\\n')\n",
    "    print(\"Runtime:\", round(end_time - start_time), \"seconds\" + '\\nTime sleep: ', time_sleep, 'seconds')\n",
    "    return df_append_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830952e",
   "metadata": {},
   "source": [
    "Una volta definite le funzioni per procedere con lo scraping, ci saranno due faso di scraping:\n",
    "1. Scraping delle offerte di lavoro \n",
    "2. Scraping delle aziende che offrono un lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86d661",
   "metadata": {},
   "source": [
    "### 1.Scraping delle offerte di lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cff3a8",
   "metadata": {},
   "source": [
    "Questa prima fase si occuperà di fare scraping sulle offerte di lavoro relative ad un determinato link.\n",
    "(es [offerte lavoro amsterdam]('https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0'))\n",
    "\n",
    "\n",
    "Se il link presenta più pagine relative alle offerte cercate, il sistema andrà a fare lo scraping su tutte le pagine sino ad arrivare alla 30°.  \n",
    "E' possibile limitare il numero di pagine sui fare scraping con l'argomento n_page, di default vengono prese tutte le pagine disponibile (max 30).  \n",
    "Dopo aver avviato la navigazione web il sistema rifiuterà i cookies e inizierà lo scraping per tutte le pagine presenti nel link.  \n",
    "Una volta completato lo scraping verrà creato un dataframe con le informazioni raccolte. Il dataframe presenterà queste colonne:\n",
    "- company: nome dell'azienda\n",
    "- job_title: Posizione lavorativa offerta\n",
    "- location: Sede dell'offerta di lavoro\n",
    "- company_rating: rating dell'azienda (secondo gli utenti di Glassdoor)\n",
    "- job_age: giorni passati dalla pubblicazione dell'offerta\n",
    "- job_link: link dell'azienda che ha pubblicato il lavoro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e09e68",
   "metadata": {},
   "source": [
    "Si farà adesso lo scraping delle prime due pagine del link [offerte lavoro amsterdam]('https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_scraping = scraping_job_page(url, n_page = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_scraping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be58291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con questa cella è possibile salvare il file nella cartella data\n",
    "df_job_scraping['run_date'] = data_file\n",
    "\n",
    "\n",
    "file_name = file + '_job_' + data + '.csv'\n",
    "df_job_scraping.to_csv('../data/' + file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94026bb0",
   "metadata": {},
   "source": [
    "### 2. Scraping delle aziende che offrono un lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b40625",
   "metadata": {},
   "source": [
    "Una volta completato la prima fase di scraping, e' possibile passare alla seconda.  \n",
    "\n",
    "Questa seconda fase prevede la ricerca di informazioni delle aziende che offrono un lavoro, queste verrano cercate nella sezione dedicata creata da Glassdoor.  \n",
    "\n",
    "Si compierà uno scraping delle informazioni generali e delle valutazioni date dagli utenti di Glassdoor.  \n",
    "\n",
    "\n",
    "Per fare questo utilizziamo la colonna \"job_link\" (*link dell'azienda che ha pubblicato il lavoro*) che trasformiamo in una lista da passare alla funzione \"scraping_company_page\".\n",
    "\n",
    "\n",
    "Una volta completato lo scraping verrà creato un dataframe con le informazioni raccolte. Il dataframe presenterà queste colonne:\n",
    "- Opportunità di carriera (rating)\n",
    "- Stipendio e benefit (rating)\n",
    "- Cultura e valori (rating)\n",
    "- Dirigenti senior (rating)\n",
    "- Equilibrio lavoro/vita privata (rating)\n",
    "- Sede centrale\t\n",
    "- Dimensioni\t\n",
    "- Fondata nel\t\n",
    "- Tipo\t\n",
    "- Settore Segmento\t\n",
    "- Entrate\n",
    "- job_link (chiave primaria)\n",
    "\n",
    "Sarà possibile unire i due dataframe con la chiave job_link.\n",
    "\n",
    "Anche questa funzione prende l'argomento n_page per limitare il numero di pagine sui cui fare scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = file + '_job_' + data + '.csv'\n",
    "#df_job_scraping = pd.read_csv('../data/' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_overview = '../data/company_overview.csv'\n",
    "df_company_overview = pd.read_csv(path_overview)\n",
    "old_links = list(df_company_overview['job_link'].unique())\n",
    "links = list(df_job_scraping['job_link'].unique())\n",
    "new_links = list(set(links) - set(old_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275458d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scraping_company=pd.DataFrame()\n",
    "df_fitt=pd.DataFrame()\n",
    "n = 5\n",
    "start_to=0\n",
    "start_time = time.time()\n",
    "for i in range(start_to, len(links), n):\n",
    "    try:\n",
    "        df_fitt = scraping_company_page(links, end_n_page = i+n, start_n_page=i)\n",
    "        df_scraping_company = df_scraping_company.append(df_fitt).reset_index(drop=True)\n",
    "        print(len(df_scraping_company))\n",
    "    except:\n",
    "        print(len(df_scraping_company))\n",
    "        pass\n",
    "end_time = time.time()\n",
    "print(\"Runtime:\", round(end_time - start_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973652af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con questa cella è possibile salvare il file nella cartella data\n",
    "df_scraping_company = df_scraping_company.rename(columns={'Sede centrale':'sede_centrale',\n",
    "                                'Dimensioni':'dimensioni',\n",
    "                                'Fondata nel' : 'fondata_nel',\n",
    "                                'Tipo':'tipo',\n",
    "                                'Entrate':'entrate',\n",
    "                                # job_link\n",
    "                                'Opportunità di carriera': 'oppurtunita_carriera',\n",
    "                                'Stipendio e benefit':'stipendio_e_benefit',\n",
    "                                'Cultura e valori':'cultura_e_valori',\n",
    "                                'Dirigenti senior':'dirigenti_senior',\n",
    "                                'Equilibrio lavoro/vita privata':'equilibrio_lavoro_vita_privata',\n",
    "                                'Settore':'settore',\n",
    "                                'Segmento':'segmento'})\n",
    "#file_name = file + '_company_' + data + '.csv'\n",
    "#df_scraping_company.to_csv('../data/' + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company_overview_new = df_company_overview.append(df_scraping_company).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company_overview_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_company_overview_new.to_csv(path_overview, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c78a2",
   "metadata": {},
   "source": [
    "E' possibile adesso mergiare i due file ed ottenere una il dataset finale da poter valutare dal punto di vista qualitativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc54a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_job_scraping.merge(df_company_overview_new, how='left', on='job_link')\n",
    "df_final['company_rating'] = df_final['company_rating'].str.extract(r'>\\s*([0-9.]+)\\s*<')\n",
    "# con questa cella è possibile salvare il file nella cartella data\n",
    "float_columns = [\n",
    "        'company_rating','oppurtunita_carriera', 'stipendio_e_benefit',\n",
    "       'cultura_e_valori', 'dirigenti_senior',\n",
    "       'equilibrio_lavoro_vita_privata']\n",
    "string_columns = [\n",
    "    'company', 'job_title', 'location', 'job_age',\n",
    "       'job_link', 'sede_centrale', 'dimensioni', 'tipo', 'settore', 'segmento', 'entrate']\n",
    "integer_columns = ['fondata_nel']\n",
    "\n",
    "for e in float_columns:\n",
    "    df_final[e] = df_final[e].astype(float)\n",
    "\n",
    "for e in string_columns:\n",
    "    df_final[e] = df_final[e].astype(str)\n",
    "    \n",
    "for e in integer_columns:\n",
    "    df_final[e] = df_final[e].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d22e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = file + '_' + data + '.csv'\n",
    "#df_final.to_csv('../data/' + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
