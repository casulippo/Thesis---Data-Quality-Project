{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "493a6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install getuseragent\n",
    "# ! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a268a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "from datetime import date\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "# BeautifulSoup e Request\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from getuseragent import UserAgent\n",
    "\n",
    "# selenium\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be91df",
   "metadata": {},
   "source": [
    "La data ingestion è un processo che consiste nell'ottenere e importare dati, per utilizzo immediato o l'archiviazione in un database o altri sistemi di gestione dei dati.\n",
    "\n",
    "L'obiettivo è quello di collezionare le informazioni delle offerte di lavoro nel modo più dettagliato possibile sito www.glassdoor.com ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb9d6a",
   "metadata": {},
   "source": [
    "### Definizione delle funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b7e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "useragent = UserAgent()\n",
    "\n",
    "theuseragent = useragent.Random()\n",
    "headers = {'User-Agent': theuseragent}\n",
    "header = {\n",
    "    \"user-agent\": theuseragent ,\n",
    "    'referer':'https://www.google.com/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5684ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allow_cookies(driver):\n",
    "    print('Decline Cookies')\n",
    "    button_setting_cookies = driver.find_element(By.CSS_SELECTOR, \"button.cookie-setting-link\")\n",
    "    button_setting_cookies.click()\n",
    "    time.sleep(np.random.choice([x/10 for x in range(7,22)]))    \n",
    "    button_confirm_cookies = driver.find_element(By.CSS_SELECTOR, \"button.save-preference-btn-handler\")\n",
    "    #Conferma le mie scelte\n",
    "    button_confirm_cookies.click()\n",
    "\n",
    "def next_page(driver):\n",
    "    print('Changing page')\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"button.nextButton\")\n",
    "    next_button.click()\n",
    "    url = driver.current_url\n",
    "\n",
    "def parse_url(url):\n",
    "    company_name = []\n",
    "    job_title=[]\n",
    "    location=[]\n",
    "    company_rating=[]\n",
    "    job_age=[]\n",
    "    job_link=[]\n",
    "    \n",
    "    r = requests.get(url, headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    job_search_div = soup.select('div#JobSearch')[0]\n",
    "    a = job_search_div.select('div#PageBodyContents')\n",
    "    b = a[0].select('div#JobResults')\n",
    "    c = b[0].select('article#MainCol')\n",
    "    len_li = len(c[0].find_all('li', class_='react-job-listing'))\n",
    "    lis = c[0].find_all('li', class_='react-job-listing')\n",
    "    \n",
    "    len_li = len(lis)\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for e in lis[0:len_li]:\n",
    "        try:\n",
    "            company_name.append(e.find('div').find('a')['title'])\n",
    "        except:\n",
    "            company_name.append(None)\n",
    "        try:\n",
    "            job_title.append(e['data-normalize-job-title'])\n",
    "        except:\n",
    "            job_title.append(None)    \n",
    "        try:\n",
    "            location.append(e['data-job-loc'])\n",
    "        except:\n",
    "            location.append(None)\n",
    "        try:\n",
    "            company_rating.append(e.find('span', class_='css-2lqh28')) ### da controllare\n",
    "        except:\n",
    "            company_rating.append(None)\n",
    "        try:\n",
    "            job_age.append(e.find(\"div\", {\"data-test\": \"job-age\"}).text)\n",
    "        except:\n",
    "            job_link.append(None)\n",
    "        try:\n",
    "            link = \"https://www.glassdoor.it\" + e.find(\"a\", {\"data-test\": \"job-link\"}).get(\"href\")\n",
    "            job_link.append(link)\n",
    "        except:\n",
    "            job_link.append(None)\n",
    "            \n",
    "    df['company'] = company_name\n",
    "    df['job_title'] = job_title\n",
    "    df['location'] = location\n",
    "    df['company_rating']=company_rating\n",
    "    df['job_age'] = job_age\n",
    "    df['job_link']=job_link\n",
    "    \n",
    "    return df\n",
    "            \n",
    "def scraping_job_page(base_url, n_page=None):\n",
    "    start_time = time.time()\n",
    "    time_sleep = 0\n",
    "    df_append = pd.DataFrame()\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\")\n",
    "    # base_url = 'https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0'\n",
    "    driver.get(base_url)\n",
    "    a = np.random.choice([x for x in range(7,15)])\n",
    "    time_sleep = time_sleep + a \n",
    "    time.sleep(a)\n",
    "    url = driver.current_url\n",
    "    #driver.maximize_window()\n",
    "    allow_cookies(driver)\n",
    "    a = np.random.choice([x for x in range(7,15)])\n",
    "    time_sleep = time_sleep + a \n",
    "    time.sleep(a)\n",
    "    r = requests.get(url, headers=header)\n",
    "    \n",
    "    text = BeautifulSoup(r.text, 'html.parser').find('div', attrs={'class': 'paginationFooter'}).text\n",
    "    \n",
    "    int_list=[]\n",
    "    for e in re.findall(r'-?\\d+\\.?\\d*', text):\n",
    "        int_list.append(int(e))\n",
    "        \n",
    "    if n_page == None:\n",
    "        n_page = max(int_list)\n",
    "    #n_page = 5\n",
    "    print('Numero di pagine sui cui fare scraping: ' + str(n_page))\n",
    "    a = np.random.choice([x for x in range(7,15)])\n",
    "    time_sleep = time_sleep + a \n",
    "    time.sleep(a)\n",
    "    \n",
    "    for page in range(n_page):\n",
    "        print(str(page +1) + '/' + str(n_page))\n",
    "        a = np.random.choice([x for x in range(7,15)])\n",
    "        time_sleep = time_sleep + a \n",
    "        time.sleep(a)\n",
    "        next_page(driver)\n",
    "        url = driver.current_url\n",
    "        if page == 0:\n",
    "            time.sleep(5)\n",
    "            close_button = driver.find_element(By.CSS_SELECTOR, \"svg.SVGInline-svg.modal_closeIcon-svg\")\n",
    "            close_button.click()\n",
    "        df = parse_url(url)\n",
    "        df_append = df_append.append(df).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "    end_time = time.time()\n",
    "    print('Done!')\n",
    "    print('Total pages scraped:', n_page, '\\n')\n",
    "    print(\"Runtime:\", round(end_time - start_time), \"seconds\" + '\\nTime sleep:', time_sleep, 'seconds')\n",
    "        \n",
    "    return df_append\n",
    "\n",
    "\n",
    "def scraping_company_page(links, end_n_page=None, start_n_page=None):\n",
    "    start_time = time.time()\n",
    "    df_append_2 =pd.DataFrame()\n",
    "    time_sleep = 0\n",
    "    n_link = 0\n",
    "    ## lista dei link\n",
    "    \n",
    "    #base_url = 'https://www.glassdoor.it/job-listing/field-service-engineer-technician-lexas-JV_IC2802269_KO0,33_KE34,39.htm?jl=1008135983566&pos=101&ao=1110586&s=58&guid=000001861873f6d389d965e51ce717e2&src=GD_JOB_AD&t=SR&vt=w&uido=FEB79CBEB143B609D33C645CD13E2F4B&ea=1&cs=1_524e761d&cb=1675447498725&jobListingId=1008135983566&cpc=65CC663E25211861&jrtk=3-0-1goc77to1g2ev801-1goc77tp0i6id800-7cbed71e05432cdf--6NYlbfkN0CVO0F7mWis5ReNIXvK0Cy97GKSpj_H8mHyNoiV7tLwhxrGQFeFbXfrFFwDAnfvPXeiJe5SavTtAEQpKcpYVReYHZsV-4ZX7UeAkoBb0f_WCVWviQdPDhB0WcxVHddsJTu6CPWu9hRPncXvGLdy3ZffF5b3aOd7vp19QcNQdw0qQd1bkijbQHvL2CZX_Cxp4BGS1Sk8JgAjiz75HrAHRuR5hA9kjnxafzWfGAAOJBSKybBbJtFcKCvWC2Py0-IgF36KHcIY5QbFzm8TqI0WJJ75VyN8D93fcG7Ikeu9ECT-vBbPKtsVv7AOpI6elTa0KTJqDzeYeI39a8bHa5BUUOjrQ1rsJFwMGLJrLrbqXOIObHs1pSu_fpr-FcNbNcmvwdLQufgm_hOEka1AR5pS2jw3Kd3MEOLDniaBdQ1tk-tgoNuL5lhZNjQUe648ZgRuUWrLaIX3oHWCh15jyNqf9dfjjRB70aXgFCI2nC397dI72pLnPhLIqqOKxPTEV4El04FFBbjsdOPDp3q0LQurvyiZmQB21o9E3WEoCcol53Fx2g0SIc5ixunqT-a4RIeTz8CzvY7DdPxvqKSbekvmaTBa_UWB3gbtD9Y%253D&ctt=1675447519123'\n",
    "       \n",
    "    \n",
    "    if start_n_page == None:\n",
    "        start_n_page=0\n",
    "    if end_n_page == None:\n",
    "        end_n_page=len(links)\n",
    "        \n",
    "    print('link to scrape: ', end_n_page - start_n_page) \n",
    "    \n",
    "    for base_url in links[start_n_page:end_n_page]: \n",
    "        n_link += 1\n",
    "        driver = webdriver.Chrome(r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\")\n",
    "        driver.get(base_url)\n",
    "        a = np.random.choice([x for x in range(7,15)])\n",
    "        time_sleep = time_sleep + a \n",
    "        time.sleep(a)\n",
    "        ## cookies\n",
    "        allow_cookies(driver)\n",
    "        a = np.random.choice([x for x in range(7,15)])\n",
    "        time_sleep = time_sleep + a \n",
    "        time.sleep(a)\n",
    "        ## Mapping dei bottoni\n",
    "        general_button = driver.find_elements(By.CSS_SELECTOR, \".css-dkrzi8.e1eh6fgm0\")\n",
    "        button_mapping = {valore.text: indice for indice, valore in enumerate(general_button)}\n",
    "    \n",
    "        \n",
    "        d_valutazione={}\n",
    "        d_azienda={}\n",
    "    \n",
    "        print('check the buttons-->', n_link)\n",
    "        try:\n",
    "            general_button = driver.find_elements(By.CSS_SELECTOR, \".css-dkrzi8.e1eh6fgm0\")\n",
    "            \n",
    "        except:\n",
    "            print('no buttons found')\n",
    "            general_button = []\n",
    "        \n",
    "        \n",
    "        if len(general_button)>0:\n",
    "            \n",
    "            try:\n",
    "    \n",
    "                azienda = general_button[button_mapping['Azienda']] # Azienda\n",
    "                azienda.click()\n",
    "                pag_azienda_span = driver.find_elements(By.CSS_SELECTOR, \".css-vugejy.es5l5kg0 span.value\")\n",
    "                pag_azienda_value = driver.find_elements(By.CSS_SELECTOR, \".css-vugejy.es5l5kg0 label\")\n",
    "                \n",
    "                azienda_span = []\n",
    "                azienda_value = []\n",
    "                \n",
    "                for e in pag_azienda_span:\n",
    "                    azienda_span.append(e.text)\n",
    "                \n",
    "                for e in pag_azienda_value:\n",
    "                    azienda_value.append(e.text)\n",
    "                d_azienda = dict(zip(azienda_value, azienda_span))\n",
    "                print('scraped company page')\n",
    "            \n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                valutazione = general_button[button_mapping['Valutazione']] # Valutazione\n",
    "                #recensioni = general_button[2] # Recensioni\n",
    "                \n",
    "                ## Cambio foglio \n",
    "                valutazione.click()\n",
    "                pag_valutazione = driver.find_elements(By.CSS_SELECTOR, \".css-a7hxlj.e121l59f1\")\n",
    "                a = np.random.choice([x for x in range(7,15)])\n",
    "                time_sleep = time_sleep + a \n",
    "                time.sleep(a)\n",
    "                valutazione_1 = [pag_valutazione[0].text, pag_valutazione[2].text, pag_valutazione[4].text,pag_valutazione[6].text, pag_valutazione[8].text]\n",
    "                valutazione_2 = [pag_valutazione[1].text, pag_valutazione[3].text, pag_valutazione[5].text,pag_valutazione[7].text, pag_valutazione[9].text]\n",
    "                print('scraped evaluation page')\n",
    "    \n",
    "                d_valutazione = dict(zip(valutazione_1, valutazione_2))\n",
    "            \n",
    "            except:\n",
    "                pass  \n",
    "            # to add benefit e stipendio\n",
    "    \n",
    "                \n",
    "            \n",
    "        d = {**d_valutazione, **d_azienda}\n",
    "            \n",
    "        driver.quit()\n",
    "        \n",
    "        \n",
    "        df2 = pd.DataFrame.from_dict(d,  orient='index').T\n",
    "        df2['job_link']=base_url\n",
    "        \n",
    "        df_append_2 = df_append_2.append(df2).reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Total pages scraped:', len(links), '\\n')\n",
    "    print(\"Runtime:\", round(end_time - start_time), \"seconds\" + '\\nTime sleep: ', time_sleep, 'seconds')\n",
    "    return df_append_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830952e",
   "metadata": {},
   "source": [
    "Una volta definite le funzioni per procedere con lo scraping, ci saranno due faso di scraping:\n",
    "1. Scraping delle offerte di lavoro \n",
    "2. Scraping delle aziende che offrono un lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86d661",
   "metadata": {},
   "source": [
    "### 1.Scraping delle offerte di lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cff3a8",
   "metadata": {},
   "source": [
    "Questa prima fase si occuperà di fare scraping sulle offerte di lavoro relative ad un determinato link.\n",
    "(es [offerte lavoro amsterdam]('https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0'))\n",
    "\n",
    "\n",
    "Se il link presenta più pagine relative alle offerte cercate, il sistema andrà a fare lo scraping su tutte le pagine sino ad arrivare alla 30°.  \n",
    "E' possibile limitare il numero di pagine sui fare scraping con l'argomento n_page, di default vengono prese tutte le pagine disponibile (max 30).  \n",
    "Dopo aver avviato la navigazione web il sistema rifiuterà i cookies e inizierà lo scraping per tutte le pagine presenti nel link.  \n",
    "Una volta completato lo scraping verrà creato un dataframe con le informazioni raccolte. Il dataframe presenterà queste colonne:\n",
    "- company: nome dell'azienda\n",
    "- job_title: Posizione lavorativa offerta\n",
    "- location: Sede dell'offerta di lavoro\n",
    "- company_rating: rating dell'azienda (secondo gli utenti di Glassdoor)\n",
    "- job_age: giorni passati dalla pubblicazione dell'offerta\n",
    "- job_link: link dell'azienda che ha pubblicato il lavoro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e09e68",
   "metadata": {},
   "source": [
    "Si farà adesso lo scraping delle prime due pagine del link [offerte lavoro amsterdam]('https://www.glassdoor.it/Lavoro/amsterdam-paesi-bassi-lavori-SRCH_IL.0,21_IC3064478.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Amsterdam%2520(Paesi%2520Bassi)&context=Jobs&dropdown=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9688649",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.glassdoor.it/Lavoro/napoli-italia-lavori-SRCH_IL.0,13_IS5113.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Napoli%252C%2520Italia&context=Jobs&dropdown=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= 'scraping_all_napoli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230214'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "data = today.strftime(\"%Y%m%d\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3643bf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-2f4c8e391739>:77: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decline Cookies\n",
      "Numero di pagine sui cui fare scraping: 30\n",
      "1/30\n",
      "Changing page\n",
      "2/30\n",
      "Changing page\n",
      "3/30\n",
      "Changing page\n",
      "4/30\n",
      "Changing page\n",
      "5/30\n",
      "Changing page\n",
      "6/30\n",
      "Changing page\n",
      "7/30\n",
      "Changing page\n",
      "8/30\n",
      "Changing page\n",
      "9/30\n",
      "Changing page\n",
      "10/30\n",
      "Changing page\n",
      "11/30\n",
      "Changing page\n",
      "12/30\n",
      "Changing page\n",
      "13/30\n",
      "Changing page\n",
      "14/30\n",
      "Changing page\n",
      "15/30\n",
      "Changing page\n",
      "16/30\n",
      "Changing page\n",
      "17/30\n",
      "Changing page\n",
      "18/30\n",
      "Changing page\n",
      "19/30\n",
      "Changing page\n",
      "20/30\n",
      "Changing page\n",
      "21/30\n",
      "Changing page\n",
      "22/30\n",
      "Changing page\n",
      "23/30\n",
      "Changing page\n",
      "24/30\n",
      "Changing page\n",
      "25/30\n",
      "Changing page\n",
      "26/30\n",
      "Changing page\n",
      "27/30\n",
      "Changing page\n",
      "28/30\n",
      "Changing page\n",
      "29/30\n",
      "Changing page\n",
      "30/30\n",
      "Changing page\n",
      "Done!\n",
      "Total pages scraped: 30 \n",
      "\n",
      "Runtime: 386 seconds\n",
      "Time sleep: 336 seconds\n"
     ]
    }
   ],
   "source": [
    "df_job_scraping = scraping_job_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.glassdoor.it/Lavoro/napoli-italia-lavori-SRCH_IL.0,13_IS5113.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=&typedLocation=Napoli%252C%2520Italia&context=Jobs&dropdown=0\n"
     ]
    }
   ],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db0b368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_age</th>\n",
       "      <th>job_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lina Stores</td>\n",
       "      <td>Capo Pasticcere</td>\n",
       "      <td>Napoli</td>\n",
       "      <td>[4.1, []]</td>\n",
       "      <td>4 g</td>\n",
       "      <td>https://www.glassdoor.it/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FACILE RISTRUTTURARE S.p.a.</td>\n",
       "      <td>Ingegnere</td>\n",
       "      <td>Napoli</td>\n",
       "      <td>[2.3, []]</td>\n",
       "      <td>17 g</td>\n",
       "      <td>https://www.glassdoor.it/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contrader</td>\n",
       "      <td>Sviluppatore Java</td>\n",
       "      <td>Italia</td>\n",
       "      <td>[4.7, []]</td>\n",
       "      <td>Oltre 30 g</td>\n",
       "      <td>https://www.glassdoor.it/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LAGARDERE TRAVEL RETAIL ITALIA</td>\n",
       "      <td>Addetto Alle Vendite</td>\n",
       "      <td>Napoli</td>\n",
       "      <td>[2.9, []]</td>\n",
       "      <td>11 g</td>\n",
       "      <td>https://www.glassdoor.it/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calzedonia Holding Spa</td>\n",
       "      <td>Personale di Supporto Alle Vendite</td>\n",
       "      <td>Napoli</td>\n",
       "      <td>[3.1, []]</td>\n",
       "      <td>Oltre 30 g</td>\n",
       "      <td>https://www.glassdoor.it/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          company                           job_title  \\\n",
       "0                     Lina Stores                     Capo Pasticcere   \n",
       "1     FACILE RISTRUTTURARE S.p.a.                           Ingegnere   \n",
       "2                       Contrader                   Sviluppatore Java   \n",
       "3  LAGARDERE TRAVEL RETAIL ITALIA                Addetto Alle Vendite   \n",
       "4          Calzedonia Holding Spa  Personale di Supporto Alle Vendite   \n",
       "\n",
       "  location company_rating     job_age  \\\n",
       "0   Napoli      [4.1, []]         4 g   \n",
       "1   Napoli      [2.3, []]        17 g   \n",
       "2   Italia      [4.7, []]  Oltre 30 g   \n",
       "3   Napoli      [2.9, []]        11 g   \n",
       "4   Napoli      [3.1, []]  Oltre 30 g   \n",
       "\n",
       "                                            job_link  \n",
       "0  https://www.glassdoor.it/partner/jobListing.ht...  \n",
       "1  https://www.glassdoor.it/partner/jobListing.ht...  \n",
       "2  https://www.glassdoor.it/partner/jobListing.ht...  \n",
       "3  https://www.glassdoor.it/partner/jobListing.ht...  \n",
       "4  https://www.glassdoor.it/partner/jobListing.ht...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_job_scraping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5be58291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con questa cella è possibile salvare il file nella cartella data\n",
    "\n",
    "\n",
    "file_name = file + '_job_' + data + '.csv'\n",
    "df_job_scraping.to_csv('../data/' + file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94026bb0",
   "metadata": {},
   "source": [
    "### 2. Scraping delle aziende che offrono un lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b40625",
   "metadata": {},
   "source": [
    "Una volta completato la prima fase di scraping, e' possibile passare alla seconda.  \n",
    "\n",
    "Questa seconda fase prevede la ricerca di informazioni delle aziende che offrono un lavoro, queste verrano cercate nella sezione dedicata creata da Glassdoor.  \n",
    "\n",
    "Si compierà uno scraping delle informazioni generali e delle valutazioni date dagli utenti di Glassdoor.  \n",
    "\n",
    "\n",
    "Per fare questo utilizziamo la colonna \"job_link\" (*link dell'azienda che ha pubblicato il lavoro*) che trasformiamo in una lista da passare alla funzione \"scraping_company_page\".\n",
    "\n",
    "\n",
    "Una volta completato lo scraping verrà creato un dataframe con le informazioni raccolte. Il dataframe presenterà queste colonne:\n",
    "- Opportunità di carriera (rating)\n",
    "- Stipendio e benefit (rating)\n",
    "- Cultura e valori (rating)\n",
    "- Dirigenti senior (rating)\n",
    "- Equilibrio lavoro/vita privata (rating)\n",
    "- Sede centrale\t\n",
    "- Dimensioni\t\n",
    "- Fondata nel\t\n",
    "- Tipo\t\n",
    "- Settore Segmento\t\n",
    "- Entrate\n",
    "- job_link (chiave primaria)\n",
    "\n",
    "Sarà possibile unire i due dataframe con la chiave job_link.\n",
    "\n",
    "Anche questa funzione prende l'argomento n_page per limitare il numero di pagine sui cui fare scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de9607e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(df_job_scraping['job_link'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275458d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link to scrape:  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-2f4c8e391739>:148: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(r\"C:\\Users\\Casulippo\\Desktop\\web_chromedriver\\chromedriver.exe\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decline Cookies\n",
      "check the buttons--> 1\n",
      "scraped company page\n"
     ]
    }
   ],
   "source": [
    "df_scraping_company=pd.DataFrame()\n",
    "df_fitt=pd.DataFrame()\n",
    "n = 30\n",
    "start_to=1\n",
    "for i in range(start_to, len(links), n):\n",
    "    df_fitt = scraping_company_page(links, end_n_page = i+n, start_n_page=i)\n",
    "    df_scraping_company = df_scraping_company.append(df_fitt).reset_index(drop=True)\n",
    "    print(len(df_scraping_company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e9185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scraping_company.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973652af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con questa cella è possibile salvare il file nella cartella data\n",
    "file_name = file + '_company_' + data '.csv'\n",
    "df_scraping_company.to_csv('../data/' + file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c78a2",
   "metadata": {},
   "source": [
    "E' possibile adesso mergiare i due file ed ottenere una il dataset finale da poter valutare dal punto di vista qualitativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc54a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_job_scraping.merge(df_scraping_company, how='left', on='job_link')\n",
    "# con questa cella è possibile salvare il file nella cartella data\n",
    "file_name = file + data + '.csv'\n",
    "df_final.to_csv('../data/' + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d22e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
